{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Stream Processing\n",
    "This is the act of continuously incorporating new data to compute a result. In stream processing, the input data is unbounded and has no predetermined beginning or end. It simply forms a series of events that arrive at the stream processing system (e.g credit card transactions, clicks on a website, or sensor readings from a iot). Naturally, we can compare streaming to batch processing, in which the computation runs on a fixed-input dataset. Oftentimes, this might be a large-scale dataset in a data warehouse that contains all the historical events from an application (e.g all website visits or sensor readings for the past month). Batch processing also takes a query to compute, similar to stream processing, but only computes the result once.\n",
    "## Stream Processing Use Cases\n",
    "* **Notifications and alerting-** Given a series of events, a notification or alert should be triggered if some sort of event or series of event occurs. An example might be driving an alert to an employee at a fulfillment center that they need to geta  certain ite, froma  location in the warehouse and ship it to a customer\n",
    "* **Real-time reporting-** We use these dashboards to monitor total platform usage, system load, uptime, and even usage of new features as they are rolled out, among other applications\n",
    "* **Incremental ETL-** One of the most common streaming applications is to reduce tyhe latency companies must endure while retrieving information into a data warehouse. Spark batch jobs are often used for ETL workloads that turn raw data into a Structured format like Parquet to enable efficient queries. Using structured streaming, these jobs can incorporate new data within seconds, enabling users to query it faster downstream. In this use case, it is critical that data is processed exactly once and in a fault-tolerant manner: we do not want to lose any input data before it makes it to the warehouse, and we do not want to load the same datab twice. \n",
    "* **Update data to serve in real time-** Streaming systems are frequently used to compute data that gets served interactively by another application. For example, a web analytics product such as google analytics might continbuously track the number of visits to each page, and use a streaming system to keep these counts up to date\n",
    "* **Real-time decision making-** This involves analyzing new inputs and responding to them automatically using business logic. An example of this would be a bank that wants to automatically verify whether a new transaction on a customer's credit card represents fraud based on their recent history, and deny the transaction if deemed fraudulent\n",
    "* **Online machine Learning-** You want to train a model on a combination of streaming and historical data from multiple users. An example might be a more sophisticated aforementioned credit card transaction use case: rather than reacting with hardcoded rules based on one customer's behavior, the company may want to continuously update a model from all customers' behavior and test each transaction against it. This is the most challenging use case of the bunch for stream processing systems because it requires aggregation across multiple users, joins against static datasets, integration with machine learning libraries, and low-latency response times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Stream Processing\n",
    "For the most part, batch is much simpler to understand, troubleshoot, and write applications in for the majority of use cases. Additionally, the ability to process data in batch allows for vastly higher data processing throughput than many streaming systems. However, stream processing is essential in two cases: first, stream processing enables lower latency: when your application needs to respond quickly, you will need a streaming system that can keep state in memory to get acceptable performance. Second, stream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation\n",
    "\n",
    "\n",
    "## Challenges of Stream Processing\n",
    "* Processing out-of-order data based on application timestamps\n",
    "* Maintaining large amounts of state\n",
    "* Supporting high-data throughput\n",
    "* Processing each event exactly once despite machine failures\n",
    "* Handling load imbalance and stragglers\n",
    "* Responding to events at low latency\n",
    "* Joining with external data in other storage systems\n",
    "* Determining how to update output sinks as new events arrive\n",
    "* Writing data transactionally to output systems\n",
    "* Updating your application's business logic at runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing Design Points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record-at-a-time versus Declarative APIs\n",
    "The simplestway to design a streaming API would be just pass each event to the application and let it react using custom code. This is the approach that many early stream8ing systems such as apache storm implemented, and it has an important place when applications need full control over the processing of data.The downside of thes systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application. For example, with a record-at-a-time API, you are responsible for tracking state over long periods of time, dropping it after some time to clear up space, and responding differently to duplicate events after a failure\n",
    "As a result, many newer streaming systems provide declaractive APIs, where your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
