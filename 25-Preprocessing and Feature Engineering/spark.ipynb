{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 13:34:54 WARN Utils: Your hostname, kevin resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp0s20f3)\n",
      "22/11/09 13:34:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 13:34:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Models According to your Use Case\n",
    "To preprocess data for Spark's different advanced analytics tools, you must consider your end objective:\n",
    "* In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent tyhe features\n",
    "* In the case of recommendation, you want to get yuour data into a column of users, a column of items (say movies or books), and a column of ratings\n",
    "* In the acse of unsupervised learning,a  column of type Vector (either dense or sparse) is needed to represent the features\n",
    "* In the case of graph analytics, you will want a dataframe of vertices and a dataframe of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales = spark.read.format('csv')\\\n",
    "                .option('header', 'true')\\\n",
    "                .option('inferSchema', 'true')\\\n",
    "                .load('/home/kevin/Desktop/Big-Data-with-Pyspark/data/retail-data/by-day')\\\n",
    "                .coalesce(5)\\\n",
    "                .where('Description IS NOT NULL')\n",
    "\n",
    "fakeIntDF = spark.read.parquet(\"/home/kevin/Desktop/Big-Data-with-Pyspark/data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json('/home/kevin/Desktop/Big-Data-with-Pyspark/data/simple-ml')\n",
    "scaleDF = spark.read.parquet('/home/kevin/Desktop/Big-Data-with-Pyspark/data/simple-ml-scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 13:44:32 WARN CacheManager: Asked to cache already cached data.\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "These are functions that convert raw data in some way. This might be to create a new interaction variable (from two other variables), to normalize a column, or to simply turn it into a double to be input into a model. Transformers are primarily used in preprocessing or feature generation. Spark's transformers only include a transform method. This is because it will not change based on the input data. \n",
    "\n",
    "<img src=\"/home/kevin/Desktop/Big-Data-with-Pyspark/images/08_spark_transformer.png\">\n",
    "\n",
    "On the left is the input dataframe with a new column representing the output transformation.\n",
    "The tokenizer is an example of a transformer. It tokenizes a string, splitting on a given character, and has nothing to learn from our data; it simply applies a function. Here is an example of how a tokenizer is built to accept the input column, how it transforms the data, and then the output from that transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------+\n",
      "|         Description|Tokenizer_155d84a48675__output|\n",
      "+--------------------+------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|          [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |          [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|          [12, message, car...|\n",
      "|BLUE HARMONICA IN...|          [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|          [gumball, coat, r...|\n",
      "+--------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tkn = Tokenizer().setInputCol('Description')\n",
    "tkn.transform(sales.select('Description')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators for Preprocessing\n",
    "Another tool for preprocessing are estimators. An estimator is necessary when a transformation you would like to perform must be initialized with data or information about the input column. For example, if you wanted to scale the values in our column to have a mean of zero and unit variance, you would need to perform a pass over the entire data in order to calculate the values you would use to normalize the data to mean zero and unit variance. Here is an example of an estimator fitting to a particular input dataset, generating a transformer that is then applied to the input dataset to append a new column\n",
    "\n",
    "<img src=\"/home/kevin/Desktop/Big-Data-with-Pyspark/images/09_spark_estimator.png\">\n",
    "\n",
    "\n",
    "An example of this is StandardScaler, which scales your input column according to the range of values in that column to have a zero mean and a variance of 1 in each dimension. For that reason, it must first perform a pass over the data to create the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_1988935f6eaf__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler().setInputCol('features')\n",
    "scaler.fit(scaleDF).transform(scaleDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Properties\n",
    "All transformers require you to specify, at a minimum, the inputCol and the outputCol, which represent the column name of the input and the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Transformers\n",
    "Such as RFormula, allow you to concisely specify a number of transformations in one. These operate at a high level and allow you to avoid doing data manipulations or transformations one by one. In general, you should try to use the highest level transformers you can, in order to minimize the risk of error and help you to focus on the business problem instead of the smaller details of implementation\n",
    "\n",
    "## RFormula\n",
    "This is the easist transformer to use when you have conventionally formatted data. Spark borrows this transformer from the R language to make it simple to declaratively specify a set of transformations for your data. With this transformer, values can be either numerical or categorical and you do not need to extract values from strings or manipulate them in any way. \n",
    "The RFormula will automatically handle categorical inputs by performing one0hot encoding. With the RFormula, numeric columns will be cast to Double but will not be one-hot encoded. If the label column is of type String, it will be transformed to Double with StringIndexer\n",
    "\n",
    "\n",
    "## SQL Transformers\n",
    "It allows you to leverage sparks vast library of SQL-related manipulations. Any SELECT statement you can use in SQL is a valid transformation. The only thing you need to change is that instead of using the table name, you should use the keyword THIS. You might want to use SQLTransformer if you want to formally codify some DataFrame manipulation as a preprocessing step, or try different SQL expressions for features during hyperparameter tuning\n",
    "You might want to use an SQLTransformer in order to represent all of your manipulations on the very rawest form of your data so that you can version different variations of manipulations as transformers. This gives you the benefit of building and testing varying pipelines, all by simply swapping out transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "|           34|       6|   14768.0|\n",
      "|         1542|      30|   13094.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "                            .setStatement(\n",
    "                                \"\"\"\n",
    "                                SELECT sum(Quantity), count(*), CustomerID\n",
    "                                FROM __THIS__\n",
    "                                GROUP BY CustomerID\n",
    "                                \"\"\"\n",
    "                            )\n",
    "\n",
    "basicTransformation.transform(sales).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorAssembler\n",
    "This is a tool you use in nearly every single pipeline you create. It helps concatenate all your features unto one big vector you can then pass to an estimator. It's used typically in the last step of a machine learning pipeline and takes as input a number of columns of Boolean, Double or Vector. This is particularly useful if you're going to perform a number of manipulations using a variety of transformers and need to gather all of those results together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_526ede91cf85__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler().setInputCols(['int1', 'int2', 'int3'])\n",
    "\n",
    "assembler.transform(fakeIntDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Continuous Features\n",
    "There are two common transformers for continuous features. First, you can convert continuous features into categorical featuyres via a process called bucketing, or you can scale and normalize your features according to several different requirements. These transformers will only work on Double types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contDF = spark.range(30).selectExpr('cast(id as double)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing\n",
    "The most straightforward approach to bucketing or binning is using a Bucketizer. This will split given continuous feature into the buckets of your designation. You specify how buckets should be created via an array or list of Double values. This is useful because you may wanty to simplify the features in your dataset or simplify their representations for interpretation later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_173d5a4cb8ba__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol('id')\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+\n",
      "|  id|QuantileDiscretizer_9ea170bd25aa__output|\n",
      "+----+----------------------------------------+\n",
      "| 0.0|                                     0.0|\n",
      "| 1.0|                                     0.0|\n",
      "| 2.0|                                     0.0|\n",
      "| 3.0|                                     0.0|\n",
      "| 4.0|                                     0.0|\n",
      "| 5.0|                                     1.0|\n",
      "| 6.0|                                     1.0|\n",
      "| 7.0|                                     1.0|\n",
      "| 8.0|                                     1.0|\n",
      "| 9.0|                                     1.0|\n",
      "|10.0|                                     1.0|\n",
      "|11.0|                                     2.0|\n",
      "|12.0|                                     2.0|\n",
      "|13.0|                                     2.0|\n",
      "|14.0|                                     2.0|\n",
      "|15.0|                                     2.0|\n",
      "|16.0|                                     2.0|\n",
      "|17.0|                                     3.0|\n",
      "|18.0|                                     3.0|\n",
      "|19.0|                                     3.0|\n",
      "+----+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol('id')\n",
    "fittedBucket = bucketer.fit(contDF)\n",
    "fittedBucket.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalization\n",
    "\n",
    "### StandardScaler\n",
    "It standardizes a set of features to have zero mean and a standard deviation of 1. The flag withStd will scale the data to unit standard deviation while the flagWithMean will center the data prior to scaling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_c33ab4c5ee12__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, MaxAbsScaler, ElementwiseProduct, Normalizer\n",
    "\n",
    "sScaler = StandardScaler().setInputCol('features')\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinmaxScaler\n",
    "This will scale the values in a vector(component wise) to the proportional values on a scale from a given min value. If you specify the minimum value to be 0 and the maximum value to be 1, then the values will fall in between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_59e48dff86ce__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol('features')\n",
    "fittedMinMax = minMax.fit(scaleDF)\n",
    "fittedMinMax.transform(scaleDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbsScaler\n",
    "This scales the data by dividing each value by the maximum absolute value of this feature. All values end up between -1 and 1. This transformer does not shift or center the data at all in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_a1db2be59c62__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MaxAbsScaler().setInputCol('features')\n",
    "fittedma = scaler.fit(scaleDF)\n",
    "fittedma.transform(scaleDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElementwiseProduct\n",
    "It allows us to scale each value in a vector by an arbitrary value. For example, given the vector below and the row \"1,0.1,-1\" the output will be \"10,1.5, -20\". Naturally, the dimensions of the scaling vector must match the dimensions of the vector inside the relevant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_eb831ff4289b__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "                    .setScalingVec(scaleUpVec)\\\n",
    "                    .setInputCol('features')\n",
    "\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer\n",
    "The normalizer allows us to scale multidimensional vectors using on of several power norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_a4e74044116c__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manhattanDistance = Normalizer().setP(1).setInputCol('features')\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Categorical Features\n",
    "\n",
    "## StringIndexer\n",
    "The simplest way to index is via the StringIndexer, which maps strings to different numerical IDs. Spark's StringIndexer also creates metadata attached to the DataFrame that specify what inputs correspond to what outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer().setInputCol('lab').setOutputCol('labelInd')\n",
    "idxRes = indexer.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    35|14.386294994851129|     5.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     4.0|\n",
      "|  red| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    45| 38.97187133755819|     6.0|\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    35|14.386294994851129|     5.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     4.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valIndexer = StringIndexer().setInputCol('value1').setOutputCol('valueInd')\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Indexed Values Back to Text\n",
    "When inspecting your machine learning results, you're likely going to want back to the original values. Since MLlib classifcation models make predictions using the indexed values this conversion is useful for converting model predictions (indices) back to the original categories. We can do this with IndexToString."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_f3a602786c2c__output|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "labelReverse = IndexToString().setInputCol('labelInd')\n",
    "labelReverse.transform(idxRes).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Transformers\n",
    "\n",
    "## Tokenizing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, NGram, CountVectorizer, HashingTF, IDF, Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |Tokens                               |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[skulls, , water, transfer, tattoos] |\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |\n",
      "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token = Tokenizer().setInputCol('Description').setOutputCol('Tokens')\n",
    "tokenized = token.transform(sales.select('Description'))\n",
    "tokenized.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |Tokens                               |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[skulls, water, transfer, tattoos]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |\n",
      "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex = RegexTokenizer()\\\n",
    "            .setInputCol('Description')\\\n",
    "            .setOutputCol('Tokens')\\\n",
    "            .setPattern(\" \")\\\n",
    "            .setToLowercase(True)\n",
    "\n",
    "\n",
    "regex.transform(sales.select('Description')).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------------------+\n",
      "|         Description|              Tokens|StopWordsRemover_34be675e390b__output|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|                 [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|                 [gumball, coat, r...|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopWords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stops = StopWordsRemover()\\\n",
    "                    .setStopWords(stopWords)\\\n",
    "                        .setInputCol('Tokens')\n",
    "\n",
    "stops.transform(tokenized).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|              Tokens|NGram_bb8d152d9182__output|\n",
      "+--------------------+--------------------------+\n",
      "|[rabbit, night, l...|      [rabbit night, ni...|\n",
      "|[doughnut, lip, g...|      [doughnut lip, li...|\n",
      "|[12, message, car...|      [12 message, mess...|\n",
      "|[blue, harmonica,...|      [blue harmonica, ...|\n",
      "|[gumball, coat, r...|      [gumball coat, co...|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram = NGram().setInputCol('Tokens').setN(1)\n",
    "bigram = NGram().setInputCol('Tokens').setN(2)\n",
    "\n",
    "bigram.transform(tokenized.select('Tokens')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Words into Numerical Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|              Tokens|            countVec|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|(500,[150,185,212...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|(500,[462,463,491...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|(500,[35,41,166],...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|(500,[10,16,36,35...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|(500,[228,281,407...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\\\n",
    "            .setInputCol('Tokens')\\\n",
    "            .setOutputCol('countVec')\\\n",
    "            .setVocabSize(500)\\\n",
    "            .setMinTF(1)\\\n",
    "            .setMinDF(2)\n",
    "\n",
    "\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spark': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8bbc17edfc6fec1dafd662e4d54fee75495278ea7d904ef532688e2f0ecb6da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
