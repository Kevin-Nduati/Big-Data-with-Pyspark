{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 09:19:01 WARN Utils: Your hostname, kevin resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp0s20f3)\n",
      "22/11/01 09:19:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 09:19:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Structured Spark Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames vs Datasets\n",
    "Dataframes are untyped while datasets are typed. To say that dataframes are untyped is aslightly inaccurate; they have types, but spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime. Datasets, on the other hand, check whether types conform to the specification at compile time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "They represent a simple type like integer or string, a complex type like an array or map, or a null value. Spark tracks all of this type information for you and offers a variety of ways, wicth which you can transform columns\n",
    "\n",
    "## Rows\n",
    "A row is nothing but a row of data. We can create these rows manually from sql, from RDDs, from data sources, or manually from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Structured API Execution\n",
    "We will demonstrate how code is executed across a cluster. Let us walk through the execution of a single structured API query from user code to executed code\n",
    "* Write DataFrame/Dataset/SQL code\n",
    "* If valid code, spark converts this to a logical plan\n",
    "* Spark transforms this logical plan to a physical plan, checking for optimizations along the way\n",
    "* Spark then executes this physical plan (RDD manipulations) on the cluster\n",
    "\n",
    "Written code is then submitted to spark either through the console or via a submitted job. This code then passes through the catalyst optimizer, which decides how the code should be executed and lays out a plan for doing so before, finally, the code is run and the result is returned to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Planning\n",
    "<img src=\"/home/kevin/Desktop/Big-Data-with-Pyspark/04-Structured API Overview/images/01-logical planning.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logical plan only represents a set of abstract transformations that do not refer to executors or drivers, it's purely to convert the user's set of expressions into the most opptimized version. It does this by converting the user code into an unresolved logical plan. This plan is unresolved because although your code might be valid, the tabl;es or columns that it refers to might not exist. Spark uses the catalog, a repository of all table and dataframe information, to resolve columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the required table ir column name does not exist in the catalog. If the analyzer can resolve it, the result is passed through the catalyst optimizer, a collection of rules that attempt to optimize the logical plan by pushing down predicates or selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Planning\n",
    "After successfully creating an optimized logical plam, spark then begins the physical planning process. The physical plan, often called a spark plan, specifies how the logical plan will execute in the cluster by generating different physical execution strategies and comparing them through a cost model. An example of the cost comparison might be choosing how to perform  given join by looking at the physical attributes of a given table (how big the table is or how big its partitions are)\n",
    "<img src=\"/home/kevin/Desktop/Big-Data-with-Pyspark/04-Structured API Overview/images/03-Physical_plan.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical planning results ina  serioes of RDDs and transformatio0ns. This result is why you might have heard of spark as a compiler - it takes queries in dataframes, datasets and sql and compiles them into RDD transformations for you\n",
    "\n",
    "## Execution\n",
    "Upon selecting a physical plan, spark runs all of this code over RDDs, the lower-level programming interface of spark. Spark performs further optimizations at runtime, generating native java bytecode that can remove entire tasks or stages during execution. Finally, the result is returned to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spark': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8bbc17edfc6fec1dafd662e4d54fee75495278ea7d904ef532688e2f0ecb6da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
