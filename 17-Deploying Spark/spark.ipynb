{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to Monitor\n",
    "\n",
    "\n",
    "## Driver and Executor Processes\n",
    "You need to monitor the driver since this is where all of the state of your application lives, and you'll have to ensure that it is running in a stable manner. \n",
    "\n",
    "## queries, Jobs, Stages and Tasks\n",
    "Sometimes we need to debug what's going on at the level of a specific query. This information allows us to know exactly what is running on the cluster at a given time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Logs\n",
    "One of the most detailed ways to monitor spark is through its log files. One challenge however, is that Python won't be able to integrate directly with Spark's Java-based logging library. Using Python's logging module or event simple print statements will still print the results to standard error and make them easy to find. \n",
    "To change spark's log level, simply run the following command\n",
    "\n",
    "```\n",
    "spark.sparkContext.setLogLevel('INFO')\n",
    "```\n",
    "\n",
    "This will allow you to read the logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Spark UI\n",
    "This provides a visual way to monitor applications while they arer running as well as metrics about your spark workload, at the spark and JVM level. Every sparkContext running launches a web UI, by default on port 4040, that displays useful information about the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(is_glass=None, count=1454),\n",
       " Row(is_glass=True, count=12861),\n",
       " Row(is_glass=False, count=527594)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\\\n",
    "        .option('header', 'true')\\\n",
    "        .csv('/home/kevin/Desktop/Big-Data-with-Pyspark/data/retail-data/all/online-retail-dataset.csv')\\\n",
    "        .repartition(2)\\\n",
    "        .selectExpr(\"instr(Description, 'GLASS' ) >= 1 as is_glass \")\\\n",
    "        .groupBy('is_glass')\\\n",
    "        .count()\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spark': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8bbc17edfc6fec1dafd662e4d54fee75495278ea7d904ef532688e2f0ecb6da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
