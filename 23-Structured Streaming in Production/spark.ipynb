{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 11:52:24 WARN Utils: Your hostname, kevin resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp0s20f3)\n",
      "22/11/09 11:52:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 11:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Tolerance and CheckPointing\n",
    "The most important operational concern for a streaming application is failure recovery. Faults are inevitable: you're going to lose a machine ina  cluster, a schema will change by accident without proper migration, or you amy even intentionally restart the cluster or application. In any of these cases, structured streaming allows you to recover an application by just restarting it. To do this, you must configure the application to use checkpointing and write-ahead logs, both of which are handled automatically by the engine. Specifically, you must configure a query to write to a checkpoint location on a reliable file system,\n",
    "structured Streaming will then periodically save all relevant progress information as well as the current intermediuate state values to thc checkpoint location. In a failure scenario, you simply need to restart your application, amking sure to point to the same location, and it will automatically recover its state and start processing data where it left off. \n",
    "To use checkpointing, specify your checkpointy location before starting your application through the checkpointLocation option on writeStream. You can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static = spark.read.json('/home/kevin/Desktop/Big-Data-with-Pyspark/data/activity-data')\n",
    "\n",
    "# streaming = spark\\\n",
    "#                 .readStream\\\n",
    "#                 .schema(static.schema)\\\n",
    "#                 .option('maxFilesPerTrigger', 10)\\\n",
    "#                 .json('/home/kevin/Desktop/Big-Data-with-Pyspark/data/activity-data')\\\n",
    "#                 .groupBy('gt')\\\n",
    "#                 .count()\n",
    "\n",
    "\n",
    "# query = streaming\\\n",
    "#                 .writeStream\\\n",
    "#                 .outputMode('complete')\\\n",
    "#                 .option('checkpointlocation', '/some/python/location')\\\n",
    "#                 .queryName('test_python_stream')\\\n",
    "#                 .format('memory')\\\n",
    "#                 .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Your Application\n",
    "Checkpointing is the most important thing to enable in order to run your applications in production. This is because the checkpoint will store all of the information about what your stream has processes thus far and what the intermediate state it may be storing is. However, checkpointing does come with a small catch - you're going to have to reasonb about your old checkpoint data when you update your streaming application. When you update your application, you're going to have to ensure your update is not breaking change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating your Streaming Application Code\n",
    "Structured Streaming is designed to allow certain types of changes to the application code between application restarts. Most importantly, you're allowed to change user-defined functions (UDFs) as long as they have the same type of signature. For example, imagine that your application starts receiving a new type of data, and one of the data parsing functions in your current logic crashes. With structured streaming, you can recompile the application with a new version of that function and pick up at the same point in the stream where it crashed earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spark': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8bbc17edfc6fec1dafd662e4d54fee75495278ea7d904ef532688e2f0ecb6da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
