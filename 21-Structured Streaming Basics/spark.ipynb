{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions\n",
    "Transformations and actions are the same, with a few restrictions. The restrictions usually involve some type of queries that the engine cannot incrementalize yet\n",
    "\n",
    "\n",
    "## Input Sources\n",
    "Structured streaming supports several input sources for reading in a streaming fashion. As at the moment, the supported input sources are:\n",
    "* Apache kafka\n",
    "* Files on a distributed file system like HDFS or S3\n",
    "* A socket source for testing\n",
    "\n",
    "## Sinks\n",
    "Just as sources allow you to get data into Structured Streaming, sinks specify the destination for the result set of that stream. Sinks and the execution engine are also responsible for reliably tracking the exact progress of data processing. Here are the supported sinks\n",
    "* Apache kafka\n",
    "* Almost any file format\n",
    "* A foreach sink for running arbitrary computation o0n the output records\n",
    "* A console sink for testing\n",
    "* A memory sink for debugging\n",
    "\n",
    "\n",
    "## Output Modes\n",
    "Defining a sink for our structured streaming job is only half the story. We alkso need to define how we want spark top write data to that sink. For instance, do we want to append new information? Do we want to update rows as we receive more information about them over time? Do we want to completely overwrite the result set everytime? To do this, we define output modes in the static structured APIs. The supported modes are as follows:\n",
    "* Append(only add new records to the output sink)\n",
    "* Update ( update changed records in place)\n",
    "* Complete (rewrite the full output)\n",
    "\n",
    "Certain queries, and certain sinks, only support certain output modes. For example, suppose that your job is just performing a map on a stream. The output data will grow indefinitely as new records arrive, so it would not make sense to use complete mode, which requires writing all the data to a new file at once. In contrast, if you are doing an aggregation into a limited number of keys, complete and update modes would make sense, but append would not, because the values of some keys need to be updated over time\n",
    "\n",
    "\n",
    "## Triggers\n",
    "Wheareas output modes define how data is output, triggers define when data is output, that is, when structured streaming should check for new input data and update its result. By default, it usually looks for new data as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results. However, this behavior can lead to writing many small output files when the sink is a set of files. Thus, spark also supports triggers based on processing time\n",
    "\n",
    "## Event-Time Processing\n",
    "Structured Streaming also has support for event-time processing (i.e processing data based on timestamps included in the record that may arrive but out of order).\n",
    "\n",
    "### event-time data\n",
    "This means time fields are embedded in your data. This means that rather than processing data according to the time it rteaches your system, you process it according to the time that it wa sgenerated, even if the records arrive out of order at the streaming application due to slow uploads or network delays. Because the system views the input data as a table, the event time is just another field in that table, and your application can do grouping, aggregation, and windowing using standard SQL operators. However, under the hood, structured streaming can take some special actions when it knows that one of your columns is an event-time field, including query optimization or detrming when it is safe to forget state about a time window. Many of these actions can be controlled using watermarks\n",
    "\n",
    "### Watermarks\n",
    "They are a feature of streaming systems that allow you to specify how late they expect to se data in event time. For example, in an application that processes logs from mobile devices, one might expect logs to be upto 30 minutes late due to upload delays. Systems that support event time, including structured streaming, usually allow setting watermarks to limit how long they need to remember old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structured Streaming in Action\n",
    "We will work with the Heterogeneity Human Activity Recognition Dataset. The dataset consists of smartphone and smartwatch sensor readings from a variety of devices, sampled at the highest frequency supported by the devices. Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking and so on. There are several different smartphones and smartwatchges used, and nine total users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Arrival_Time', LongType(), True), StructField('Creation_Time', LongType(), True), StructField('Device', StringType(), True), StructField('Index', LongType(), True), StructField('Model', StringType(), True), StructField('User', StringType(), True), StructField('gt', StringType(), True), StructField('x', DoubleType(), True), StructField('y', DoubleType(), True), StructField('z', DoubleType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static = spark.read.json('/home/kevin/Desktop/Big-Data-with-Pyspark/data/activity-data/')\n",
    "dataSchema = static.schema\n",
    "dataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "|1424686735691|1424688581745026978|nexus4_2|  145|nexus4|   g|stand|-3.814697E-4|   0.0184021|-0.013656616|\n",
      "|1424686735890|1424688581945252808|nexus4_2|  185|nexus4|   g|stand|-3.814697E-4|-0.031799316| -0.00831604|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema)\\\n",
    "                            .option('maxFilesPerTrigger', 1)\\\n",
    "                            .json('/home/kevin/Desktop/Big-Data-with-Pyspark/data/activity-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxFilesPerTrigger allows you to control how quickly spark will read all of the files in tyhe folder. By specifying this value lower, we're artificially limiting the flow of the stream to one file per trigger. This helps us to demonstrate how structured streaming runs incrementally in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy('gt').count()\n",
    "\n",
    "\n",
    "# set partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our transformation, we need only to specify the action tp start the query. We will specify the output destination, or output sink for our result of this query. For this example, we will write to a memory sink which keeps an in-memory table of the results. In the process of specifying this sink, we're going to need to define how spark will output that data. In this example, we use the complete output mode. This mode reqrites all of the keys along with their counts after every trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 23:43:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fa8858a5-a042-4f58-90bd-c9e8b3ac2a7e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/08 23:43:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "activityQuery = activityCounts.writeStream\\\n",
    "                                .queryName('activity_counts')\\\n",
    "                                .format('memory')\\\n",
    "                                .outputMode('complete')\\\n",
    "                                .start()\n",
    "\n",
    "\n",
    "# activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must specify that we would like to wait for the termination of the query using awaitTermination() to prevent the driver process from exiting while the query is active. This must be included for production applications, otherwise, your stream won't be able to run. Spark lists this stream, and other active ones in \n",
    "```\n",
    "spark.streams.active\n",
    "```\n",
    "\n",
    "Now that we have the stream running, we can experimenbt with the result by querying the in-memory table it is maintaining of the current output of our streaming aggregation. To see the current data in this output table, we simply need to query it. We will simply loop and print the results of the streaming query every second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|24619|\n",
      "|     stand|22769|\n",
      "|stairsdown|18729|\n",
      "|      walk|26512|\n",
      "|  stairsup|20905|\n",
      "|      null|20896|\n",
      "|      bike|21593|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|61547|\n",
      "|     stand|56924|\n",
      "|stairsdown|46825|\n",
      "|      walk|66280|\n",
      "|  stairsup|52260|\n",
      "|      null|52239|\n",
      "|      bike|53984|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit| 98471|\n",
      "|     stand| 91079|\n",
      "|stairsdown| 74922|\n",
      "|      walk|106048|\n",
      "|  stairsup| 83614|\n",
      "|      null| 83584|\n",
      "|      bike| 86377|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|135392|\n",
      "|     stand|125234|\n",
      "|stairsdown|103010|\n",
      "|      walk|145816|\n",
      "|  stairsup|114975|\n",
      "|      null|114931|\n",
      "|      bike|118773|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations on Streams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selections and Filterings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 00:02:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-57edf201-8ade-4fc6-b831-9a72090a4176. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/09 00:02:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "simpleTransform = streaming.withColumn('stairs', expr(\"gt like '%stairs%'\"))\\\n",
    "                            .where('stairs')\\\n",
    "                            .where('gt is not null')\\\n",
    "                            .select('gt', 'model', 'arrival_time', 'creation_time')\\\n",
    "                            .writeStream\\\n",
    "                            .queryName('simple_transform')\\\n",
    "                            .format('memory')\\\n",
    "                            .outputMode('append')\\\n",
    "                            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 00:06:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5ebaec1c-bc17-4570-aee8-c57fe0c350c3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/09 00:06:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube('gt', 'model').avg()\\\n",
    "                            .drop(\"avg(Arrival_time)\")\\\n",
    "                            .drop(\"avg(Creation_time)\")\\\n",
    "                            .drop(\"avg(Index)\")\\\n",
    "                            .writeStream\\\n",
    "                            .queryName('device_counts')\\\n",
    "                            .format('memory')\\\n",
    "                            .outputMode('complete')\\\n",
    "                            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  null| -5.4943324403959E-4|2.791446281700071E-4|-2.33994461689892...|\n",
      "|      walk|nexus4|-0.00390116006094...|0.001052508689953...|-6.95435553042998...|\n",
      "|      walk|  null|-0.00390116006094...|0.001052508689953...|-6.95435553042998...|\n",
      "|  stairsup|  null|-0.02479965287771643|-0.00800392344379...|-0.10034088415060415|\n",
      "|     stand|  null|-3.11082189691727...|3.218461665975321...|2.141300040636463...|\n",
      "|      bike|  null|0.022688759550866838|-0.00877912156368...|-0.08251001663412372|\n",
      "|  stairsup|nexus4|-0.02479965287771643|-0.00800392344379...|-0.10034088415060415|\n",
      "|      null|nexus4|4.796918779024287E-4|-0.00601540958963...|-0.01013356489164...|\n",
      "|      null|  null|4.796918779024287E-4|-0.00601540958963...|-0.01013356489164...|\n",
      "|stairsdown|  null|0.021613908669165335|-0.03249018824752615| 0.12035922691504052|\n",
      "|      null|  null|-0.00847688860109...|-7.30455258739179...|0.003090601491419903|\n",
      "|       sit|nexus4| -5.4943324403959E-4|2.791446281700071E-4|-2.33994461689892...|\n",
      "|stairsdown|nexus4|0.021613908669165335|-0.03249018824752615| 0.12035922691504052|\n",
      "|     stand|nexus4|-3.11082189691727...|3.218461665975321...|2.141300040636463...|\n",
      "|      null|nexus4|-0.00847688860109...|-7.30455258739179...|0.003090601491419903|\n",
      "|      bike|nexus4|0.022688759550866838|-0.00877912156368...|-0.08251001663412372|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  null| -5.4943324403959E-4|2.791446281700071E-4|-2.33994461689892...|\n",
      "|      walk|nexus4|-0.00390116006094...|0.001052508689953...|-6.95435553042998...|\n",
      "|      walk|  null|-0.00390116006094...|0.001052508689953...|-6.95435553042998...|\n",
      "|  stairsup|  null|-0.02479965287771643|-0.00800392344379...|-0.10034088415060415|\n",
      "|     stand|  null|-3.11082189691727...|3.218461665975321...|2.141300040636463...|\n",
      "|      bike|  null|0.022688759550866838|-0.00877912156368...|-0.08251001663412372|\n",
      "|  stairsup|nexus4|-0.02479965287771643|-0.00800392344379...|-0.10034088415060415|\n",
      "|      null|nexus4|4.796918779024287E-4|-0.00601540958963...|-0.01013356489164...|\n",
      "|      null|  null|4.796918779024287E-4|-0.00601540958963...|-0.01013356489164...|\n",
      "|stairsdown|  null|0.021613908669165335|-0.03249018824752615| 0.12035922691504052|\n",
      "|      null|  null|-0.00847688860109...|-7.30455258739179...|0.003090601491419903|\n",
      "|       sit|nexus4| -5.4943324403959E-4|2.791446281700071E-4|-2.33994461689892...|\n",
      "|stairsdown|nexus4|0.021613908669165335|-0.03249018824752615| 0.12035922691504052|\n",
      "|     stand|nexus4|-3.11082189691727...|3.218461665975321...|2.141300040636463...|\n",
      "|      null|nexus4|-0.00847688860109...|-7.30455258739179...|0.003090601491419903|\n",
      "|      bike|nexus4|0.022688759550866838|-0.00877912156368...|-0.08251001663412372|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(2):\n",
    "    spark.sql(\"SELECT * FROM device_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output\n",
    "This section dives deeper into the details of how sources, sinks and output modes work in structured streaming. Specifically, we discuss how, and when, and where data flows into and out of the system. \n",
    "\n",
    "\n",
    "## Where Data is Read and Written (Sources and Sinks)\n",
    "Structured Streaming suppoirts several production sources and sinks (files and apache kafka), as well as some debugging tools like the memory sink table. \n",
    "\n",
    "### File Source and sink\n",
    "Probably the simplest source is tyhe simplest file source. It's easy to reason about and understand. These are csv, parquet, json etc. The only difference between the file source/sink and spark's file source is that with streaming, we can control the number of files that we read in during each trigger via the maxFilesPerTrigger option. Any files you add into an input directory for a streaming job need to appear in it atomically. Otherwise, spark will process partially written files before you have finished\n",
    "\n",
    "### Kafka source and sink\n",
    "Apache kafka is a distributed publish-and-subscribe system for streams of data. Kafka lets you to publish and subscribe to streams of records like you might do with a message queue - these are stored in a fault-tolerant way. Think of kafka like a distributed buffer. Kafka lets you store streams of records in categories that are referred to as topics. Each record in kafka consists of a key, value, and a timestamp. Topics consist of immutable sequences of records for which the position of a record in a sequence is called an offset. Reading data is called subscribing to a topic and writing data is as simple as publishing to a topic\n",
    "Spark allows you to read from kafka with both batch and streaming DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from the kafka source\n",
    "To read, you first need to choose one of the following options: assign, subscribe, or subscribePattern. Only one of these can be present as an option when yoiu go to read from kafka. Assign is a fine-grained way of specifying not just the topic but also the topic partitions from which you would like to read. This is specified as a JSON string {\"topicA\"[0,1], \"topicB\": [2,4]}. Subscribe and subscribePattern are ways of subscribing to one or more topics either by specifying a list of topics (former) or via a pattern (latter).\n",
    "Second, you need to specify the kafka.bootstrap.servers that kafka provides to connect with the service. After you have specified your options, you have several other options to specify:\n",
    "* **startingOffsets and endingOffsets-** The start point when a query is started, either earliest, which is from the earliest offsets; latest which is from the latest offsets; or a json string specifying a starting offset for each TopicPartition\n",
    "* **failOnDataLoss-** Whether to fail the query when it's possible that data is lost (e.g topics are deleted, or offsets are out of range). This might be a false alarm. You can disable it when it doesn't work as you expected\n",
    "* **maxOffsetsPerTrigger-** The total number of offsets to read in a given trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # subscribe to 1 topic\n",
    "# df1 = spark.readStream.format(\"kafka\")\\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "#   .option(\"subscribe\", \"topic1\")\\\n",
    "#   .load()\n",
    "\n",
    "# # subscribe to multiple topics\n",
    "# df2 = spark.readStream.format('kafka')\\\n",
    "#             .option('kafka.bootstrap.servers', 'host1:port1, host2:port2')\\\n",
    "#             .option('subscribe', 'topic1, topic2')\\\n",
    "#             .load()\n",
    "\n",
    "# # subscribe to a pattern\n",
    "# df3 = spark.readStream.format('kafka')\\\n",
    "#             .option('kafka.bootstrap.servers', 'host1:port1, host2:port2')\\\n",
    "#             .option('subscribePattern', 'topic.*')\\\n",
    "#             .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the source will have the following schema:\n",
    "* key:binary\n",
    "* value:binary\n",
    "* topic:string\n",
    "* partition:int\n",
    "* offset: long\n",
    "* timestamp: long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Data is Output (Output Modes)\n",
    "There are 3 modes supported by structured streaming:\n",
    "\n",
    "### Append mode\n",
    "This is the default behavior and the simplest to understand. When new rows are added to the result table, they will be output to the sink based on the trigger that you specify. This mode ensured that each row is output once (and only once), assuming that you have a fault-tolerant sink. When you use append mode with event-time and watermarks, only the final result will output to the sink\n",
    "\n",
    "### Complete mode\n",
    "This will output the entire state of the result table to your output sink. This is useful when you're working with some stateful data for which all rows are expected to change over time or the sink you are writing does not support row-level updates. \n",
    "\n",
    "### Update mode\n",
    "This is similar to complete mode except that only the rows that are different from the previous ones are written out to the sink. Naturally, your sink must support row-level updates to support this mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 01:24:45 WARN StateStore: Error running maintenance thread\n",
      "java.lang.IllegalStateException: SparkEnv not active, cannot do maintenance on StateStores\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:596)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:582)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:442)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spark': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8bbc17edfc6fec1dafd662e4d54fee75495278ea7d904ef532688e2f0ecb6da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
